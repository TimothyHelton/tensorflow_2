{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow\n",
    "## Chapter 10 - Introduction to Artificial Neural Networks with Keras\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import concurrent.futures\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, InputLayer\n",
    "\n",
    "from tensorflow_2 import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = utils.package_dir() / 'data'/ 'ch10'\n",
    "LOG_DIR = DATA_DIR / 'logs'\n",
    "TRAIN_DIR = DATA_DIR / 'train'\n",
    "VAL_DIR = DATA_DIR / 'val'\n",
    "TEST_DIR = DATA_DIR / 'test'\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_logdir(desc: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Generate path to new run log directory.\n",
    "    \n",
    "    :param desc: run description\n",
    "    :return: log file path with timestamp and optional description\n",
    "    \"\"\"\n",
    "    return LOG_DIR / time.strftime(f'{desc}_%Y_%m_%d_%H_%M_S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_full, y_train_full), (x_test, y_test) = (\n",
    "    tf.keras.datasets.fashion_mnist.load_data()\n",
    "    )\n",
    "print(f'Train Shape: {x_train_full.shape}')\n",
    "print(f'Train Data Type: {x_train_full.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\n",
    "    't-shirt_top',\n",
    "    'trouser',\n",
    "    'pullover',\n",
    "    'dress',\n",
    "    'coat',\n",
    "    'sandal',\n",
    "    'shirt',\n",
    "    'sneaker',\n",
    "    'bag',\n",
    "    'ankle_boot',\n",
    "    )\n",
    "classes_idx = {n: v for n, v in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Validation Stratified Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train_full,\n",
    "    y_train_full,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=y_train_full\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=y_train, name='Train'))\n",
    "fig.add_trace(go.Histogram(x=y_val, name='Validation'))\n",
    "fig.add_trace(go.Histogram(x=y_test, name='Test'))\n",
    "\n",
    "fig.update_traces(opacity=0.7)\n",
    "fig.update_layout(\n",
    "    title_text='Dataset Distributions',\n",
    "    xaxis=dict(\n",
    "        title='Class',\n",
    "        tickvals=tuple(classes_idx.keys()),\n",
    "        ticktext=tuple(classes_idx.values()),\n",
    "    ),\n",
    "    yaxis_title_text='Count',\n",
    "    bargroupgap=0.1,\n",
    "    barmode='group'\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data to Files\n",
    "\n",
    "This will more mimic an actual use case where the images are too large to be held in memory.\n",
    "\n",
    "To use the following structure:\n",
    "```\n",
    "data_dir/\n",
    "  train_dir/\n",
    "    class_0/\n",
    "      #.jpg\n",
    "      #.jpg\n",
    "    ...\n",
    "    class_n/\n",
    "      #.jpg\n",
    "      #.jpg\n",
    "  val_dir/\n",
    "    class_0/\n",
    "      #.jpg\n",
    "      #.jpg\n",
    "    ...\n",
    "    class_n/\n",
    "      #.jpg\n",
    "      #.jpg\n",
    "  test_dir/\n",
    "    class_0/\n",
    "      #.jpg\n",
    "      #.jpg\n",
    "    ...\n",
    "    class_n/\n",
    "      #.jpg\n",
    "      #.jpg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = (\n",
    "    (x_train, y_train, TRAIN_DIR),\n",
    "    (x_val, y_val, VAL_DIR),\n",
    "    (x_test, y_test, TEST_DIR),\n",
    "    )\n",
    "\n",
    "for x, y, directory in datasets:\n",
    "    print(f'Saving Dataset Images: {directory}')\n",
    "    # create directories\n",
    "    for label in np.unique(y):\n",
    "        (directory / classes_idx[label]).mkdir(parents=True,\n",
    "                                               exist_ok=True)\n",
    "    # save images\n",
    "    with concurrent.futures.ProcessPoolExecutor() as pool:\n",
    "        futures = []\n",
    "        for im, label in zip(x, y):\n",
    "            path = (directory / classes_idx[label]\n",
    "                    / f'{hashlib.sha256(im).hexdigest()}.png')\n",
    "            if not path.is_file():\n",
    "                futures.append(pool.submit(cv2.imwrite, str(path), im))\n",
    "        for f in concurrent.futures.as_completed(futures):\n",
    "            f.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor = 1 / np.iinfo(x_train.dtype).max\n",
    "\n",
    "im_gen_train = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    rescale=scale_factor,\n",
    "    )\n",
    "\n",
    "im_gen_val = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=scale_factor,\n",
    "    )\n",
    "\n",
    "train_data_gen = im_gen_train.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='sparse',\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    target_size=(28, 28),\n",
    "    )\n",
    "\n",
    "val_data_gen = im_gen_train.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='sparse',\n",
    "    seed=42,\n",
    "    target_size=(28, 28),\n",
    "    )\n",
    "\n",
    "test_data_gen = im_gen_train.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='sparse',\n",
    "    seed=42,\n",
    "    target_size=(28, 28),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(train_data_gen)\n",
    "data_gen_classes = {v: k for k, v in train_data_gen.class_indices.items()}\n",
    "for n in range(10):\n",
    "    ax = plt.subplot(2, 5, n + 1)\n",
    "    ax.imshow(x[n], cmap='gray')\n",
    "    ax.set_title(data_gen_classes[y[n]])\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = tf.keras.Sequential([\n",
    "    InputLayer(input_shape=(28, 28, 1)),\n",
    "    Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "    Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax'),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional Model\n",
    "Allows multiple inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_0 = Input(shape=(28, 28, 1), name='input_0')\n",
    "hidden_1 = Conv2D(filters=32, kernel_size=3, strides=(2, 2), activation='relu',\n",
    "                  name='hidden_1')(input_0)\n",
    "hidden_2 = Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu',\n",
    "                  name='hidden_2')(hidden_1)\n",
    "flatten = Flatten(name='flatten')(hidden_2)\n",
    "output = Dense(10, activation='softmax', name='output')(flatten)\n",
    "func_model = tf.keras.Model(inputs=[input_0], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = seq_model\n",
    "model = func_model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, to_file=DATA_DIR / 'model_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "\n",
    "<br>\n",
    "<font color=red>\n",
    "    Start TensorBoard before fitting the model.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir $LOG_DIR --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "`fit()` method\n",
    "- if dataset is skewed add `class_weight` argument\n",
    "- use `sample_weight` argument to if the reliability of the label is different per instance (experts evaluated some labels, while others were labeled by an angorithm)\n",
    "\n",
    "NOTE:\n",
    "The time when Accuracy is calculated is not the same for the train dataset as the validation dataset.\n",
    "- Validation: calculated ***end*** of each epoch\n",
    "- Training: running mean ***durring*** each epoch\n",
    "\n",
    "To compensate the training metrics should be shifted by $\\frac{1}{2}$ an epoch to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    get_run_logdir('baseline_cnn')\n",
    "    )\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    epochs=64,\n",
    "    verbose=4,\n",
    "    validation_data=val_data_gen,\n",
    "    validation_freq=4,\n",
    "    callbacks=[tensorboard_cb],\n",
    "    steps_per_epoch=2 * len(train_data_gen) // BATCH_SIZE,\n",
    "    workers=2,\n",
    "    use_multiprocessing=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Test Set\n",
    "- Estimate generalization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data_gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
