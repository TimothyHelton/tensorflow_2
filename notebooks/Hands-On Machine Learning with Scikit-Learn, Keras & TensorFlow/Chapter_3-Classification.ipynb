{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow\n",
    "\n",
    "## Chapter 2: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import (confusion_matrix, f1_score, precision_recall_curve,\n",
    "    precision_score, recall_score, roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from tensorflow_2.exceptions import InputError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../../data/ch3_classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1, data_home=DATA_DIR.parent)\n",
    "print(mnist['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist['target'] = mnist['target'].astype(np.uint8)\n",
    "x_train, y_train = [mnist[k][:60000] for k in ('data', 'target')]\n",
    "x_test, y_test = [mnist[k][60000:] for k in ('data', 'target')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(dset: str='train', idx: int=0, score: Optional[bool]=None):\n",
    "    \"\"\"\n",
    "    Plot example from dataset.\n",
    "    \n",
    "    :param dset: choose either `train` or `test`\n",
    "    :param idx: index of example\n",
    "    :param score: model predicted score\n",
    "    \"\"\"\n",
    "    if dset not in ('train', 'test'):\n",
    "        raise InputError(\n",
    "            f'dset={dset}',\n",
    "            f'Valid inputs for dset are \"train\" or \"test\"')\n",
    "    x = x_train if dset == 'train' else x_test\n",
    "    y = y_train if dset == 'train' else y_test\n",
    "    score = '' if score is None else f'   Predict: {score}'\n",
    "    plt.imshow(x[idx].reshape(28, 28), cmap='binary')\n",
    "    plt.title(f'Label: {y[idx]}{score}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "plot_example('train', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train Binary Classifier\n",
    "\n",
    "Stochastic Gradient Descent (SGD) classifier\n",
    "- capable of handling very large datasets efficiently\n",
    "- evaluates training instances independently\n",
    "    - suited for online learning\n",
    "- relies on randomness during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_value = 5\n",
    "y_train_binary = y_train == binary_value\n",
    "y_test_binary = y_test == binary_value\n",
    "\n",
    "sgd_classifier = SGDClassifier(random_state=42)\n",
    "sgd_classifier.fit(x_train, y_train_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(10):\n",
    "    score = sgd_classifier.predict([x_train[n]])[0]\n",
    "    plot_example(dset='train', idx=n, score=score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cross-Validation of Binary Classifier\n",
    "\n",
    "Algorithm\n",
    "1. Randomly split the training set in k distinct subsets called ***folds***.\n",
    "1. Train the model on k-1 folds.\n",
    "1. Evaluate the model on the one fold that was not included in training.\n",
    "1. Repeat until all folds have been used as an evaluation set.\n",
    "1. Average the results of all the trained folds.\n",
    "\n",
    "### Example implementation of Cross-Validation\n",
    "```python\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=42)\n",
    "score = []\n",
    "for train_idx, test_idx in skfolds.split(x_train, y_train):\n",
    "    clone_model = clone(model)\n",
    "    x_train_folds = x_train[train_idx]\n",
    "    y_train_folds = y_train[train_idx]\n",
    "    x_test_fold = x_train[test_idx]\n",
    "    y_test_fold = y_train[test_idx]\n",
    "    clone_model.fit(x_train_folds, y_train_folds)\n",
    "    predict = clone_model.predict(x_test_fold)\n",
    "    n_correct = sum(predict == y_test_fold)\n",
    "    score.append(n_correct / len(pedict)) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Cross-Validataion Accuracy of Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(sgd_classifier, x_train, y_train_binary, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the classifier said a two never appeared in this dataset the model would have an accuracy of 90%.\n",
    "\n",
    "<font color='red'>\n",
    "    Accuracy is generally not the preferred performance measure for classifiers, especially when dealing with *skewed* datasets.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix of Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = cross_val_predict(sgd_classifier, x_train, y_train_binary, cv=3)\n",
    "c_matrix = confusion_matrix(y_train_binary, y_train_pred)\n",
    "pd.DataFrame(c_matrix,\n",
    "             columns=['Predicted False', 'Predicted True'],\n",
    "             index=['Actual False', 'Actual True'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "    <b>\n",
    "        Increasing precision reduces recall, and vice versa (Precision/Recal trade-off)\n",
    "    </b>\n",
    "</font>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<font color='green'>\n",
    "    For the binary case: tn, fp, fn, tp = confusion_matrix().ravel()\n",
    "</font>\n",
    "\n",
    "#### Precision\n",
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "#### Recall\n",
    "$$recall = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = c_matrix.ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "    Use Scikit-Learn functions for Precision and Recall\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = [f(y_train_binary, y_train_pred)\n",
    "                     for f in (precision_score, recall_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score\n",
    "\n",
    "- Combination of precision and recall into a single metric.\n",
    "- The harmonic mean of precision and recall.\n",
    "- Metric gives much more weight to low values.\n",
    "- A high F1 score requires *both* precision and recall to be high.\n",
    "\n",
    "$$F_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}}$$\n",
    "\n",
    "$$F_1 = 2 \\left( \\frac{precision \\cdot recall}{precision + recall} \\right)$$\n",
    "\n",
    "$$F_1 = \\frac{TP}{TP + \\frac{FN + FP}{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_train_binary, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision vs. Recall\n",
    "\n",
    "- Increasing the threshold decreases recall and will generally impove precision (sometimes precision will decrease)\n",
    "- Lowering the threshold increases recall and reduces precision\n",
    "\n",
    "<font color='red'>\n",
    "    Scikit-Learn uses a default threshold of zero.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_function_scores = cross_val_predict(\n",
    "    sgd_classifier, x_train, y_train_binary, cv=3, method='decision_function'\n",
    ")\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(\n",
    "    y_train_binary, decision_function_scores\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    np.c_[precisions[:-1], recalls[:-1]],\n",
    "    index=thresholds,\n",
    "    columns=['Precision', 'Recall'],\n",
    ").rename_axis('Threshold')\n",
    "\n",
    "fig = px.line(df, title='Precision & Recall vs Threshold')\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(df, x='Recall', y='Precision', title='Precision vs Recall')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Threshold to Acheive 90% Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.9)]\n",
    "y_train_pred_90 = decision_function_scores >= threshold_90_precision\n",
    "\n",
    "precision, recall = [f(y_train_binary, y_train_pred_90)\n",
    "                     for f in (precision_score, recall_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "- `RandomForestClassifier` does not have a `decision_funtions()` method\n",
    "- use the `predict_proba()` method\n",
    "    - returns an array containing a row per instance and a column per class, each containing the probability that the given instance belongs to the given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "y_probas_rf = cross_val_predict(rf_classifier, x_train, y_train_binary,\n",
    "                                cv=3, method=\"predict_proba\")\n",
    "y_scores_rf = y_probas_rf[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the Threshold to Acheive 90% Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions_rf, recalls_rf, thresholds_rf = precision_recall_curve(\n",
    "    y_train_binary, y_scores_rf)\n",
    "treshold_90_precision_rf = thresholds_rf[np.argmax(precisions_rf >= 0.9)]\n",
    "y_train_pred_90_rf = y_scores_rf >= treshold_90_precision_rf\n",
    "\n",
    "precision_rf, recall_rf = [f(y_train_binary, y_train_pred_90_rf)\n",
    "                           for f in (precision_score, recall_score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operating Characteristic (ROC)\n",
    "\n",
    "- similar to the precision vs recall curve\n",
    "- true positive rate (TPR) vs false positive rate (FPR)\n",
    "    - TPR is another name for ***Recall***\n",
    "    - FPR is the ratio of negative instances that are incorrectly classified as positive.\n",
    "    - FPR is equal to 1 - true negative rate (TNR)\n",
    "- Sensitivity vs 1 - Specificity\n",
    "\n",
    "$$TPR = Recall = \\frac{TP}{TP + FN}$$\n",
    "$$FPR = 1 - TNR = 1 - Specificity = 1 - \\frac{TN}{TN + FP}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_train_binary, decision_function_scores)\n",
    "auc = roc_auc_score(y_train_binary, decision_function_scores)\n",
    "threshold = fpr[np.nonzero(tpr == recall)[0]]\n",
    "\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_train_binary, y_scores_rf)\n",
    "auc_rf = roc_auc_score(y_train_binary, y_scores_rf)\n",
    "threshold_rf = fpr_rf[np.nonzero(tpr_rf == recall_rf)[0]]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name='SGD', hovertext=f'AUC: {auc}'))\n",
    "fig.add_trace(go.Scatter(x=threshold, y=[recall], mode='markers', name='Threshold',\n",
    "    marker=dict(size=12,), showlegend=False))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=fpr_rf, y=tpr_rf, mode='lines', name='RF', hovertext=f'AUC: {auc_rf}'))\n",
    "fig.add_trace(go.Scatter(x=threshold_rf, y=[recall_rf], mode='markers', name='Threshold',\n",
    "    marker=dict(size=12,), showlegend=False))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Random',\n",
    "    line=dict(color='black', dash='dash'), showlegend=False))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Receiver Operating Characteristic (ROC)',\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification Process Summary\n",
    "\n",
    "1. Create an instance of a model\n",
    "1. Create binary labels for the test data\n",
    "1. Fit the model to the test data\n",
    "1. Run Cross-Validation on the test data (`cross_val_score`)\n",
    "1. Plot Confusion Matrix\n",
    "1. Calculate Precision (`precision_score`)\n",
    "1. Calculate Recall (`recall_score`)\n",
    "1. Calculate F1 (`f1_score`)\n",
    "1. Plot Precision vs Recall\n",
    "1. Choose a Threshold\n",
    "1. Plot ROC Curve with threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Multiclass Classification\n",
    "\n",
    "- also call **Multinomial Classifiers**\n",
    "\n",
    "| Model                       | Binary Classification | Multiclass Classification |\n",
    "|:--------------------------- |:---------------------:|:-------------------------:|\n",
    "| k Nearest Neighbors         | X                     | X                         |\n",
    "| naive Bayes                 | X                     | X                         |\n",
    "| Random Forest               | X                     | X                         |\n",
    "| Stochastic Gradient Descent | X                     | X                         |\n",
    "| Logistic Regression         | X                     |                           |\n",
    "| Support Vector Machine      | X                     |                           |\n",
    "\n",
    "OvR -> one-versus-the-rest classifier\n",
    "OvO -> one-versus-one\n",
    "\n",
    "Scikit-Learn will use OvR or OvO when a binary classifier is asked to perform multiclass classification.\n",
    "- `from sklearn.multiclass import OneVsRestClassifier`\n",
    "- `from sklearn.multiclass import OneVsOneClassifier`\n",
    "\n",
    "<br>\n",
    "<font color='green'>\n",
    "    The list of target classes is stored in the classes_ attribute.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Classifier\n",
    "\n",
    "<font color='red'>\n",
    "    Warning:<br>\n",
    "      45 models will be trained for the default OvO strategy.\n",
    "      If a GPU is not available the following cell will take a while to execute.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(x_train, y_train)\n",
    "len(svm_classifier.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier.predict([x_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_scores = svm_classifier.decision_function([x_train[0]])\n",
    "svm_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use OvR Stretegy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_svm = OneVsRestClassifier(SVC())\n",
    "ovr_svm.fit(x_train, y_train)\n",
    "len(ovr_svm.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_svm.predict([x_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_classifier.fit(x_train, y_train)\n",
    "sgd_classifier.predict([x_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_classifier.decision_function([x_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD with Cross-Validation\n",
    "\n",
    "<br>\n",
    "<font color='red'>\n",
    "    <b>\n",
    "        Make sure to scale the inputs!\n",
    "    </b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.astype(np.float64))\n",
    "cross_val_score(sgd_classifier, x_train_scaled, y_train, cv=3,\n",
    "                scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(rf_classifier, x_train_scaled, y_train, cv=3,\n",
    "                scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Error Analysis\n",
    "\n",
    "- Plot standardized Confusion Matrix\n",
    "    - Divide each value in the confusion matrix by the number of images in the corresponding class (rows) to view error rates instead of absolute numbers of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Multilabel Classification\n",
    "\n",
    "- Output a binary vector indicating each class as either pressent or missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
